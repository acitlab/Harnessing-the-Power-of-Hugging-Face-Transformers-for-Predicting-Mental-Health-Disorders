{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('csv', data_files='dataset_name.csv', split='train')\n",
    "dataset = dataset.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert_model_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "  return tokenizer(examples['text'], padding=\"max_length\", truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained('bert_model_name', num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve, confusion_matrix, auc\n",
    "import numpy as np\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    auc = roc_auc_score(labels, logits[:, 1])\n",
    "    fpr, tpr, _ = roc_curve(labels, logits[:, 1])\n",
    "    tn, fp, fn, tp = confusion_matrix(labels, predictions).ravel()\n",
    "\n",
    "    return {\n",
    "        \"accuracy\": accuracy_score(labels, predictions),\n",
    "        \"f1\": f1_score(labels, predictions),\n",
    "        \"auc\": auc,\n",
    "        \"roc\": (fpr, tpr),\n",
    "        \"confusion_matrix\": {\n",
    "            \"tn\": tn,\n",
    "            \"fp\": fp,\n",
    "            \"fn\": fn,\n",
    "            \"tp\": tp\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "training_args = TrainingArguments(\"test-trainer\", evaluation_strategy=\"epoch\", num_train_epochs=5)\n",
    "\n",
    "kf = KFold(n_splits=10, shuffle=True)\n",
    "\n",
    "eval_results = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(tokenized_data[\"train\"])):\n",
    "    print(\"Training fold {}...\".format(fold+1))\n",
    "\n",
    "    train_dataset = tokenized_data[\"train\"].select(train_idx)\n",
    "    val_dataset = tokenized_data[\"train\"].select(val_idx)\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=val_dataset,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    trainer.train()\n",
    "    eval_result = trainer.evaluate(eval_dataset=val_dataset)\n",
    "    eval_results.append(eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f1_scores = []\n",
    "auc_scores = []\n",
    "confusion_matrices = []\n",
    "\n",
    "for eval_result in eval_results:\n",
    "    accuracies.append(eval_result[\"eval_accuracy\"])\n",
    "    f1_scores.append(eval_result[\"eval_f1\"])\n",
    "    auc_scores.append(eval_result[\"eval_auc\"])\n",
    "    confusion_matrices.append(eval_result[\"eval_confusion_matrix\"])\n",
    "\n",
    "avg_accuracy = sum(accuracies) / len(accuracies)\n",
    "avg_f1_score = sum(f1_scores) / len(f1_scores)\n",
    "avg_auc_score = sum(auc_scores) / len(auc_scores)\n",
    "\n",
    "total_confusion_matrix = {'tn': 0, 'fp': 0, 'fn': 0, 'tp': 0}\n",
    "\n",
    "for confusion_matrix in confusion_matrices:\n",
    "    total_confusion_matrix['tn'] += confusion_matrix['tn']\n",
    "    total_confusion_matrix['fp'] += confusion_matrix['fp']\n",
    "    total_confusion_matrix['fn'] += confusion_matrix['fn']\n",
    "    total_confusion_matrix['tp'] += confusion_matrix['tp']\n",
    "\n",
    "num_folds = len(confusion_matrices)\n",
    "avg_confusion_matrix = {\n",
    "    'tn': total_confusion_matrix['tn'] // num_folds,\n",
    "    'fp': total_confusion_matrix['fp'] // num_folds,\n",
    "    'fn': total_confusion_matrix['fn'] // num_folds,\n",
    "    'tp': total_confusion_matrix['tp'] // num_folds\n",
    "}\n",
    "\n",
    "print(\"Average evaluation results over all folds:\")\n",
    "print(\"Accuracy:\", avg_accuracy)\n",
    "print(\"F1 score:\", avg_f1_score)\n",
    "print(\"AUC score:\", avg_auc_score)\n",
    "print(\"Confusion matrix:\", avg_confusion_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'my_model'\n",
    "trainer.save_model(model_name)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
